% \todo{Label Convention: Definition related to messaging should be named defn:messaging-xxx. Definition related to AE should be named defn:AE-xxx. Definition related to PIR should be named defn:PIR-xxx.  Definition specific to ASPHR should be named defn:asphr-xxx.}
\section{General Definitions}
\label{sec:general-defn}
In this section, we design a security definition for a messaging application that is as general as possible. Our definition shares many similarities with Canetti and Krawczyk's foundational CK model \cite{CK2001keyexchange} for analyzing key exchange protocols over untrusted channels. However, we find it difficult to adapt the CK models, especially the authenticated-links model(AM), directly to account for metadata privacy. Therefore, we design our security definition from scratch.

We start from the following basic principles.
\begin{enumerate}
    \item The messaging system has a centralized server in charge of storing and routing messages. We do not consider decentralized messaging systems in this paper.
    \item The messaging system has a large number of users, interacting with "client" software on their computers. The client software should allow the user to register, add friends, and send messages at any time. It should display received messages to the user.
    \item To achieve metadata privacy, the messaging application should hide metadata of conversations between honest users from a powerful adversary that controls the server and a subset of clients. 
\end{enumerate}
We now translate these principles into mathematical definitions that describe a general messaging system.
\begin{definition}
\label{defn:messaging-timestep}
A \textbf{timestep} is a basic unit of time in our system. This is different from ``rounds" used in most MPM security definitions, since clients do not necessarily transmit real or fake messages at every timestep. Instead, a timestep plays a similar role as a clock cycle in computer hardware â€” think of it as being 1 nanosecond. We assume that the system starts on timestep $t = 1$. Methods are executed on positive integer timesteps.
\end{definition}
\begin{definition}
\label{defn:messaging-client-view}
The \textbf{view} of a client is a tuple $(\cF, \cM)$ consisting of

1. A list of friends $\cF$ of the client.

2. A list of messages $\cM$ received by the client, including the sender and content of the messages.
\end{definition}
\textbf{Remark}: For simplicity, the view does not include sent messages. This is because the frontend GUI can simply store such messages locally and display them to the user.
\begin{definition}
\label{defn:messaging-registration-info}
The \textbf{registration information}, denoted $\reg$ in this paper, is the unique identifier of a user.
\end{definition}
\textbf{Remark: }Throughout the rest of the paper, we will always use the registration info as the identifier in the messaging system. For example, we will make friends with and send messages to a registration info. Registration info is ubiquitous in practical messaging systems: in Messenger, it is the Facebook handle. In Signal, it is the phone number. In Anysphere, it is the ``public ID" as defined in \cite[Figure 6]{whitepaper}.
\begin{definition}
\label{defn:messaging-user-input}
A \textbf{user input} is a command the user can issue to the client. In our current protocol, it can take one of the following values.
\begin{itemize}
    \item $\emptyset$: noop.
    \item $\trust(\reg)$: Add the user identified by $\reg$ as a friend, and enable the two parties to start a conversation.
    \item $\send(\reg, \msg)$: Send the message $\msg$ to the client identified by $\reg$. We assume that $\msg$ always has a constant length $L_{\msg}$.\footnote{For simplicity, we assume this holds even for adversarial inputs.}
\end{itemize}

\todo{We could do a ``setparameter" method here to allow clients to customize their transmission schedule, etc.}

Without loss of generality, we assume each user issues exactly one input per timestep.
\end{definition}
\textbf{Remark}:  In our implementation, we take $L_{\msg} \approx 1\mathsf{KB}$. To support variable length messages, we pad short messages and split long messages into chunks of length $L_{\msg}$. This modification does not affect our security definition below.
\begin{definition}
\label{defn:messaging-scheme}
A \textbf{Messaging System} consists of the following polynomial time algorithms.

Client Side Algorithms for the stateful client $C$.
\begin{itemize}
    \item $C.\Register(1^{\lambda}, i, N) \to \reg$. The client registration algorithm takes in a security parameter $\lambda$, the index $i$ of the client, the total number of users $N$, and outputs a public registration info $\reg$. It also initializes client storage and keypairs.
    
    \item $C.\Userinput(t, \cI) \to \req$. This algorithm handles a user input $\cI$. It updates the client storage to reflect the new input, then issues a (possibly empty) request $\req$ to the server.
    
    \item $C.\Serverrpc(t, \resp)$. This algorithm handles the server's response $\resp$ and updates client storage.
    
    \item $C.\mathsf{GetView}() \to V$. This algorithm outputs the view of the client (\cref{defn:messaging-client-view}). Its output is passed to the GUI and displayed to the user.
\end{itemize}

Server Side Algorithms for the stateful server $S$.

\begin{itemize}
    \item $S.\mathsf{InitServer}(1^{\lambda}, N)$. This algorithm takes in the security parameter $\lambda$, number of clients $N$, and initializes the server side database $D_S$.
    \item $S.\Clientrpc(t, \{\req_i\}_{i = 1}^N) \to \{\resp_i\}_{i = 1}^N$. This algorithm responds to all client requests $\req_i$ the server received on a given timestep $t$. It outputs the responses $\resp_i$ that gets sent back to the client.
\end{itemize}
\end{definition}


Now we know what a messaging system is, we can describe some desired properties. In the rest of this section, we will look at three properties we wish Anysphere to satisfy: Correctness, Metadata privacy, and Integrity.

\subsection{Correctness}
\label{subsec:messaging-correctness}
First, we describe how the server and clients interact when the application is running normally. We call this scenario the Honest Server Experiment.

\begin{definition}  \hfill\\
\label{defn:messaging-honest-server-experiment}
The honest server experiments takes the following parameters

\begin{enumerate}
    \item $\lambda$, the security parameter.
    \item $N$, the number of clients, bounded above by a polynomial function $N(\lambda)$ of $\lambda$.
    \item $T$, the number of timesteps, bounded above by a polynomial function $T(\lambda)$ of $\lambda$.
    \item For each client $i \in [N]$ and timestep $t \in [T]$, a user input $\cI_{i, t}$.
\end{enumerate}

Let $S$ denote the server machine, let $\{C_i\}_{i = 1}^N$ denote the client machines. The experiment is described below.
\begin{figure}[ht!]
\begin{framed}
\textbf{Honest Server Experiment}
\begin{enumerate}
\item $S.\mathsf{InitServer}(1^{\lambda}, N)$. 
\item For each $i \in [N]$, $\reg_i \leftarrow C_i.\Register(1^{\lambda}, i, N)$. 
\item For $t$ from $1$ to $T$:
    \begin{enumerate}
    \item For each $i \in [N]$, $\req_i \leftarrow C_i.\Userinput(t, \cI)$.
    
    \item $\{\resp_i\} \leftarrow S.\Clientrpc(t, \{\req_i\})$.
    
    \item For each $i \in [N]$, $C_i.\Serverrpc(t, \resp_i)$.
    \end{enumerate}
\end{enumerate}
\end{framed}
\caption{The Honest Server Experiment for Messaging System}
\label{expr:messaging-honest-server}
\end{figure}

\end{definition}

\arvid{Do we want an adversary to be able to control some of the clients? Ideally I think we would want to, because we want to guarantee resistance against denial of service attacks from clients}
\stzh{Good idea. I'll come back to this after I finish the security proof in the current iteration.}

In this case, we expect the client's view to be "correct". First, we need to define what "correct" means here. Informally speaking, the correct view should satisfy
\begin{enumerate}
    \item The list of friends contain the friends we called $\trust$ on.         
    \item Messages from a client with established trust should be present.  
    \item No other friends or messages should be present. 
\end{enumerate}
We state the formal definition.
\begin{definition}
\label{defn:client-view-correct}
Given a set of clients identified by $\{\reg_i\}_{i \in [N]}$, and user inputs $\{\cI_{i, t}\}_{i \in [N], t \in [T]}$, a view $(\cF_j, \cM_j)$ of client $j$ is \textbf{correct} if it satisfies
\begin{multline*}
\cF_j \cap \{\reg_i\}_{i \in [N]} = \{\reg_k: \exists t \in [T], \trust(\reg_k) = \cI_{j, t}\},    
\end{multline*}
and
\begin{align*}
 \cM_j = \{(\reg_k, \msg): &\exists t, \send(\reg_j, \msg) = \cI_{k, t} \land \\
 &\exists t' < t, \trust(\reg_j) = \cI_{k, t'} \land \\
 &\exists t'', \trust(\reg_k) = \cI_{j, t''}\}.   
\end{align*}
\end{definition}
\textbf{Remark}: Note that we allow $\cF$ to contain friends that are non-existent. Ruling out these friends is a part of the trust establishment mechanism (\cite[Section 4]{whitepaper}), which is beyond the scope of this paper. 

In the definition, if user $k$ tries to send user $j$ before user $j$ adds user $k$ as a friend, user $j$ should be able to receive the message. This is a feature of our system. 

Since the GUI can query the client at any time, we expect the client's view to be ``correct" all the time. There is a caveat: due to the lack of synchronous rounds, the clients do not immediately read all messages sent by their friends. Thus, the strongest correctness notion of sequential consistency might not be satisfied. Instead, we settle for a pair of weaker consistency models defined in \cite{doug13Consistency}. 
\begin{definition}
\label{defn:messaging-correctness}
 We say a messaging system is \textbf{correct} if it satisfies the following consistency model.
 
 Take any choice of parameters of the honest world experiment, any $j \in [N]$, and any positive integer $T_0 \leq T$. Let $V_j \leftarrow C_j.\mathsf{GetView}()$ be the view of client $j$ after timestep $T_0$ of the Honest World Experiment. Then we must have

1) \textbf{Consistent Prefix}: $V$ is identical to the correct view of the client $j$ if a prefix of user inputs have been executed on each client machine. More formally, with probability $1 - \negl(\lambda)$, for any $j \in [N]$, there exists a map $t: [N] \to [T_0]$ such that  $V$ is a correct view of client $j$ under inputs $(\{\reg_i\}_{i \in [N]}, \{\cI'_{i, t}\})$
where we define
$$\cI'_{i, t} = \begin{cases}
\cI_{i, t}, t \leq t(i) \\
\emptyset, t > t(i)
\end{cases}.$$
2) \textbf{Eventual Consistency}: For any $T_1$, there is a polynomial function $T_{cons} = T_{cons}(N, T_1)$ such that if $T_0 \geq T_{cons}$, then with probability $1 - \negl(\lambda)$, we can take $t(i) \geq T_1$ for every $i \in [N]$.
\end{definition}

\subsection{Metadata Security and Integrity}
\label{subsec:messaging-security-integrity}
We now turn to security definitions. We write our security definitions following the real world-ideal world paradigm used in \cite[Section 2.2]{shi2021non}. On a high level, the real world experiment is the honest world experiment with an adversarial server who can run arbitrary code. The ideal world experiment is the real world definition with crucial information "redacted". We say the messaging system is secure if the views of the adversary under the two experiments are indistinguishable.

We first define the real world experiment.

\begin{definition}
\label{defn:messaging-real-world-experiment}
The real world experiment use the parameters $\lambda, N, T$ as in \cref{defn:messaging-honest-server-experiment}. Furthermore, let $\cA$ be a stateful p.p.t. adversary. Let $\cH$ denote the set of honest clients the adversary chooses. Denote $\reg_{\cH} = \{\reg_i\}_{i \in\cH}$ to be the registration info of honest clients. The experiment $\Real^{\cA}(1^{\lambda})$ is described in \cref{expr:messaging-real-world}.
\end{definition}

\begin{figure}[!ht]
\begin{framed}
\textbf{Real World Experiment}
\begin{enumerate}
\item $\cH \leftarrow \cA(1^{\lambda}, N, T)$.
\item For each $i \in \cH$, $\reg_i \leftarrow C_i.\Register(1^{\lambda}, i, N)$. 
\item $\{\cI_{i, 1}\} \leftarrow \cA(1^{\lambda}, \reg_{\cH})$.
\item For $t$ from $1$ to $T$:
    \begin{enumerate}
    \item For each $i \in H$, $\req_i \leftarrow C_i.\Userinput(t, \cI_{i, t})$.
    
    \item $\{\resp_i\}_{i \in \cH}, \{\cI_{i, t + 1}\}_{i \in \cH} \leftarrow \cA(1^{\lambda}, \{\req_i\}_{i \in \cH})$.
    
    \item For each $i \in \cH$, $C_i.\Serverrpc(t, \resp_i)$.
    \end{enumerate}
\end{enumerate}
\end{framed}
\caption{Real World Experiment for Messaging System}
\label{expr:messaging-real-world}
\end{figure}

We next define the ideal world experiment. As in \cite{shi2021non}, we first define the leakage, which is the information the adversary is allowed to know. Informally, it contains the time and contents of
\begin{enumerate}
    \item Trust establishment with compromised clients.
    \item Messages sent to the compromised clients.
\end{enumerate}
The formal definition is below.

\begin{definition}
\label{defn:messaging-leakage}
Let $\reg_\cH$ be the registration info of honest clients. Let $\{\cI_{i, t}\}_{i \in \cH, t \in [T]}$ be the input from honest clients. We define the \textbf{Leakage} $\mathsf{Leak}(\{\cI_{i, t}\}, \reg_{\cH})$ as
$$\Leak(\{\cI_{i, t}\}, \reg_\cH) = \{\cI_{i, t}: (i, t) \in \Leak_f \cup \Leak_m\}$$
where
$$\Leak_f = \{(i, t): \trust(\reg) = \cI_{i, t}, \reg \notin \reg_{\cH}\}.$$
$$\Leak_m = \{(i, t): \mathsf{SendMessage}(\reg, \msg) = \cI_{i, t}, \reg \notin \reg_{\cH}\}.$$
\end{definition}
% \arvid{let's use other symbols than $\cF$ and $\cM$ because we use them for the view of a single client already and this is similar but not the same}
% \stzh{Resolved.}
\begin{definition}
\label{defn:messaging-ideal-world-experiment}
We use the same parameters and notations as \cref{defn:messaging-real-world-experiment}. Furthermore, let $\Sim$ be a stateful simulator. The ideal-world experiment $\mathsf{Ideal}^{\cA, \Sim}(1^{\lambda})$ is described in \cref{expr:messaging-ideal-world}.
\begin{figure}[h]
\begin{framed}
\textbf{Ideal World Experiment}
\begin{enumerate}
\item $\cH \leftarrow \cA(1^{\lambda}, N, T)$.
\item $\reg_{\cH} = \{\reg_i\}_{i \in \cH} \leftarrow \Sim(1^{\lambda}, N, T)$. 
\item $\{\cI_{i, 1}\}_{i \in \cH} \leftarrow \cA(1^{\lambda}, \reg_{\cH})$.
\item For $t$ from $1$ to $T$:
    \begin{enumerate}
    \item  $\{\req_i\}_{i \in \cH} \leftarrow \Sim(t, \mathsf{Leak}(\{\cI_{i, t}\}, \reg_{\cH}))$.
    
    \item $\{\resp_i\}_{i \in \cH}, \{\cI_{i, t + 1}\}_{i \in \cH} \leftarrow \cA(1^{\lambda}, \{\req_i\}_{i \in \cH})$.
    
    \item $\Sim(t, \{\resp_i\}_{i \in \cH})$.
    \end{enumerate}
\end{enumerate}
\end{framed}
\caption{Ideal World Experiment for Messaging System}
\label{expr:messaging-ideal-world}
\end{figure}

\end{definition}

Finally, we can state our definition of a metadata secure messaging system.

\begin{definition}
\label{defn:messaging-security}
We say a messaging system is \textbf{SIM-metadata secure} if for any $N$ and $T$ polynomially bounded in $\lambda$, there exists a p.p.t simulator $\Sim$ such that for any p.p.t adversary $\cA$, the view of $\cA$ is indistinguishable in $\mathsf{Real}^{\cA}(1^{\lambda})$ and $\mathsf{Ideal}^{\cA, \mathsf{\Sim}}(1^{\lambda})$.
\end{definition}

% Arvid's notes because this confused him for several hours.
% Think of \mathcal{A} as the Distinguisher's right hand! Or, another way of saying it â€” it is an adversary *against* the Simulator, NOT the adversary that is trying to attack the system in the wild.
% Suppose X is an NSA spy (i.e., real world adversary). Then, the security definition has shown us that the Simulator is a really really good simulator â€” even when you try super hard to break it, you cannot. Hence, X's view in the real world will be the same as X's view in a world where everyone else is simulated by the Simulator. This means that X might as well be running in a vacuum, and hence cannot ever learn any information whatsoever.

The simulator based definition is a relatively new way of writing security definitions. For readers more accustomed to indistinguishability-based security definitions, we include an equivalent version below.
\begin{definition}
\label{defn:messaging-ind-experiment}
We use the same notations as in \cref{defn:messaging-ideal-world-experiment}. For parameter $b \in \{0, 1\}$, the IND experiment $\Ind_b^{\cA}$ is described below, 
\begin{figure}[!h]
\begin{framed}
\textbf{IND Experiment}
\begin{enumerate}
\item $\cH \leftarrow \cA(1^{\lambda}, N, T)$.
\item For each $i \in \cH$, $\reg_i \leftarrow C_i.\Register(1^{\lambda}, i, N)$. 
\item $\{\cI^{0}_{i, 1}\}, \{\cI^{1}_{i, 1}\} \leftarrow \cA(1^{\lambda}, \{\reg_i\}_{i \in \cH})$.
\item For $t$ from $1$ to $T$:
    \begin{enumerate}
    \item For each $i \in H$, $\req_i \leftarrow C_i.\Userinput(t, \cI^{b}_{i, t})$.
    
    \item for $b'\in \{0, 1\}$, $\{\resp^{b'}_i\}_{i \in \cH},\{\cI^{b'}_{i, t + 1}\}_{i \in \cH} \leftarrow \cA(1^{\lambda}, \{\req_i\}_{i \in \cH})$.
    
    \item For each $i \in \cH$, $C_i.\Serverrpc(t, \resp^{b}_i)$.
    \end{enumerate}
\end{enumerate}
\end{framed}
\caption{IND Experiment for Messaging System}
\label{expr:messaging-IND}
\end{figure}

We say the adversary $\cA$ is \textbf{admissible} if with probability $1$ we have
\begin{align*}
\Leak(\{\cI^{0}_{i, t}\}_{i \in \cH, t \in [T]}, \{\reg_i\}_{i \in \cH}) \\
= \Leak(\{\cI^{1}_{i, t}\}_{i \in \cH, t \in [T]}, \{\reg_i\}_{i \in \cH}).    
\end{align*}
\end{definition}
\begin{definition}
\label{defn:messaging-IND-security}
We say a messaging system is \textbf{IND-metadata secure} if for any $N$ and $T$ polynomially bounded in $\lambda$, and any admissible adversary $\cA$, the IND experiments $\Ind_0^{\cA}$ and $\Ind_1^{\cA}$ are indistinguishable.
\end{definition}

Using the argument in \cite[Appendix A]{shi2021non}, we can show that IND-metadata security is equivalent to SIM-metadata security.

Finally, we define the notion of integrity. Informally, while a malicious server can DoS users, it shouldn't be able to forge messages  or selectively omit messages between honest users. In other words, the system must guarantee consistent prefix, but not eventual consistency.
\begin{definition}
\label{defn:messaging-integrity}
We run the same real-world experiment in \cref{defn:messaging-real-world-experiment}. For any pair of honest users $i, j \in \cH$, define $V_j = (\cF, \cM) \leftarrow C_j.\mathsf{GetView}()$ at the end of the experiment. Then we say the messaging system \textbf{guarantees integrity} if with probability $1 - \negl(\lambda)$, there exists a $t(i) \in [T]$ such that
\begin{align*}
     &\{\msg: &&(\reg_i, \msg) \subset \cM\} \\
      =& \{\msg: &&\exists t \leq t(i), \send(\reg_j, \msg) = \cI_{i, t} \land \\
          &   &&\exists t' < t, \trust(\reg_j) = \cI_{i, t'} \land \\
          &   &&\exists t'', \trust(\reg_i) = \cI_{j, t''}\}.
\end{align*}
\end{definition}

\subsection{A weaker Security Definition}
\label{subsec:messaging-security-weaker}
Unfortunately, Anysphere does not support the strongest security notion in \cref{defn:messaging-security}. In fact, the CF attack paper \cite{angel2018cf} argues that this security notion is very hard to satisfy in general. For the threat model in our whitepaper \cite{whitepaper}, we argued security based on the strong assumption that no friends are compromised. In reality, this assumption is almost impossible to guarantee. In this section, we formally define what it means for no friends to be compromised. We also define a weaker security notion which allows a small number of compromised friends, yet still theoretically guarantees security.

\arvid{Another path to explore here would be to create a new leak function, say LeakCF, which contains the exact information leaked for the CF attack. For example, one potentially useful leak function would be one that contains the current number of friends, or perhaps the number of friends but rounded to the nearest 10. In the worst case (such as our current prioritization case), we would leak the timing that a message actually gets sent to the server, which might be very hard to model here... Hmmmm}

\stzh{I agree. This remains unresolved. We need to figure out what to do here.}

\begin{definition}
\label{defn:messaging-security-weaker}
We say that a set of input $\{\cI_{i, t}\}_{i \in \cH, t \in [T]}$ satisfy \textbf{no compromised friends} if for any $i \in \cH, j \in \cK$ and $t \in [T]$, we have
$$\trust(\reg_j) \neq \cI_{i, t}.$$

We say that a set of input $\{\cI_{i, t}\}_{i \in \cH, t \in [T]}$ satisfy \textbf{$B$-bounded friends} if for any $i \in \cH$, the set
$$\{\reg: \exists t, \trust(\reg) = \cI_{i, t}\}$$
has cardinality at most $B$.

We say a messaging scheme is correct with $B$-bounded friends if for any parameters $(\lambda, N, T, \{\cI_{i, t}\})$ of the honest server experiment, if $\{\cI_{i, t}\}$ satisfy $B$-bounded friends, then the scheme produces the correct views.

We say a messaging scheme is SIM-secure with no compromised friends / $B$-bounded friends if for any polynomial upper bounds on $N$ and $T$, there exists a p.p.t simulator $\Sim$ such that for any p.p.t adversary $\cA$, the view of $\cA$ is indistinguishable in $\mathsf{Real}^{\cA}(1^{\lambda})$ and $\mathsf{Ideal}^{\cA, \mathsf{\Sim}}(1^{\lambda})$, provided that the input set $\{\cI_{i, t}\}_{i \in \cH, t \in [T]}$ satisfies no compromised friends / $B$-bounded friends.
\end{definition}
The Anysphere core protocol described below satisfies SIM-security under with either $B$-bounded friends or no compromised friends model. The no compromised friends case essentially follows from \cite[Appendix C]{angel2018thesis}. On the other hand, the $B$-bounded friends case is much more subtle. The next section explains why.