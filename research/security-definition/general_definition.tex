\todo{General comment: standardize labeling convention, and name the 20+ experiments}
\section{General Definitions}
\label{sec:general-defn}
In this section, we design a security definition for a messaging application that is as general as possible. We start from the following basic principles.
\begin{enumerate}
    \item The messaging system has a centralized server in charge of storing and routing messages. We do not consider decentralized messaging systems in this paper.
    \item The messaging system has a large number of users, interacting with ``client" software on their computers. The client software should allow the user to register, add friends, and send messages at any time. It should display received messages to the user.
    \item To achieve metadata privacy, the messaging application should hide metadata of conversations between honest users from a powerful adversary that controls the server and a subset of clients. 
\end{enumerate}
We now translate these principles into mathematical definitions that describe a general messaging system.
\begin{definition}
\label{defn:timestep}
A \textbf{timestep} is a basic unit of time in our system. This is different from ``rounds" used in most MPM security definitions, since clients do not necessarily transmit real or fake messages at every timestep. Instead, a timestep plays a similar role as a clock cycle in computer hardware. We assume that the system starts on timestep $t = 1$. Methods are executed on positive integer timesteps.
\end{definition}
\begin{definition}
\label{defn:client-view}
The \textbf{view} of a client is a tuple consisting of

1. A list of friends $\cF$ of the client.

2. A list of messages $\cM$ received by the client, including the sender and content of the messages.
\end{definition}
\begin{definition}
The \textbf{registration information}, denoted $\reg$ in this paper, is the unique identifier of a user.
\end{definition}
\textbf{Remark: }Throughout the rest of the paper, we will always use the registration info as the identifier in the messaging system. For example, we will makes friends with and send messages to a registration info. Registration info is ubiquitous in practical messaging systems: in Messenger, it is the Facebook handle. In Signal, it is the email address. In Anysphere, it is the ``public ID" as defined in \cite[Figure 6]{whitepaper}.
\begin{definition}
\label{defn:user-input}
A \textbf{user input} is a command the user can issue to the client. In our current protocol, it can take one of the following values.
\begin{itemize}
    \item $\emptyset$: noop.
    \item $\trust(\reg)$: Add the user identified by $\reg$ as a friend, and enable the two parties to start a conversation.
    \item $\send(\reg, \msg)$: Send the message $\msg$ to the client identified by $\reg$. We assume that $\msg$ has a constant length $L_{\msg}$.
\end{itemize}

\todo{We could do a ``setparameter" method here to allow clients to customize their transmission schedule, etc.}

Without loss of generality, we assume each user issues exactly one input per timestep.
\end{definition}
\textbf{Remark}:  In our implementation, we take $L_{\msg} = 1\mathsf{KB}$. To support variable length messages, we simply pad short messages, and split long messages into chunks of length $L_{\msg}$. This modification does not affect our security definition below.
\begin{definition}
\label{defn:messaging-scheme}
A \textbf{Messaging System/Application/App} is a scheme consisting of the following polynomial time algorithms on stated Turing machines.

Client Side Algorithms for client $C$.
\begin{itemize}
    \item $C.\mathsf{Register}(1^{\lambda}, i, N) \to \reg$. The client registration algorithm takes in a security parameter $\lambda$, the index $i$ of the client, the total number of users $N$, and outputs a public registration info $\reg$. It also initializes client storage and keypairs.
    
    \item $C.\mathsf{Input}(t, \cI) \to \req$. This algorithm handles a user input $\cI$. It updates the client storage to reflect the new input, then issues a (possibly empty) request $\req$ to the server.
    
    \item $C.\mathsf{ServerRPC}(t, \resp)$. This algorithm handles the server's response $\resp$ and updates client storage.
    
    \item $C.\mathsf{GetView}() \to V$. This algorithm outputs the view of the client(\cref{defn:client-view}). Its output is passed to the GUI and displayed to the user.
\end{itemize}

Server Side Algorithms for server $S$.

\begin{itemize}
    \item $S.\mathsf{InitServer}(1^{\lambda}, N)$. This algorithm takes in the security parameter $\lambda$, number of clients $N$, and initializes the server side database $D_S$.
    \item $S.\mathsf{ClientRPC}(t, \{\req_i\}_{i = 1}^N) \to \{\resp_i\}_{i = 1}^N$. This algorithm responds to all client requests $\req_i$ the server received on a given timestep $t$. It outputs the responses $\resp_i$ that gets sent back to the client.
\end{itemize}
\end{definition}


Now we know what a messaging system is, we can describe some desired properties. In the rest of this section, we will look at three properties we wish Anysphere to satisfy: Correctness, Metadata Security, and Integrity.

\subsection{Correctness}
First, we describe how the server and clients interact when the application is running normally. We call this scenario the Honest Server Experiment

\begin{definition}[Honest Server Experiment]  \hfill\\
\label{defn:messaging-honest-server-experiment}
The honest server experiments takes in the following parameters

\begin{enumerate}
    \item $\lambda$, the security parameter.
    \item $N$, the number of clients, bounded above by a polynomial function $N(\lambda)$ of $\lambda$.
    \item $T$, the number of timesteps, bounded above by a polynomial function $T(\lambda)$ of $\lambda$.
    \item For each client $i \in [N]$ and timestep $t \in [T]$, a user input $\cI_{i, t}$.
\end{enumerate}

Let $S$ denote the server machine, let $\{C_i\}_{i = 1}^N$ denote the client machines. The experiment is described below.
\begin{figure}[ht!]
\begin{framed}
\textbf{Honest Server Experiment}
\begin{enumerate}
\item $S.\mathsf{InitServer}(1^{\lambda}, N)$. 
\item For each $i \in [N]$, $\reg_i \leftarrow C_i.\mathsf{Register}(1^{\lambda}, i, N)$. 
\item For $t$ from $1$ to $T$:
    \begin{enumerate}
    \item For each $i \in [N]$, $\req_i \leftarrow C_i.\mathsf{Input}(t, \cI)$.
    
    \item $\{\resp_i\} \leftarrow S.\mathsf{ClientRPC}(t, \{\req_i\})$.
    
    \item For each $i \in [N]$, $C_i.\mathsf{ServerRPC}(t, \resp_i)$.
    \end{enumerate}
\end{enumerate}
\end{framed}
\label{expr:messaging-honest-server}
\caption{The Honest Server Experiment}
\end{figure}

\end{definition}

\arvid{Do we want an adversary to be able to control some of the clients? Ideally I think we would want to, because we want to guarantee resistance against denial of service attacks from clients}
\stzh{Good idea. I'll come back to this after I finish the security proof in the current iteration.}

In this case, we expect the client's view to be "correct". First, we need to define what "correct" means here. Informally speaking, the correct view should satisfy
\begin{enumerate}
    \item The list of friends contain the friends we called $\trust$ on.         
    \item Messages from a client with established trust should be present.  
    \item No other friends or messages should be present. 
\end{enumerate}
We state the formal definition.
\begin{definition}
\label{defn:client-view-correct}
Given a set of clients identified by $\{\reg_i\}_{i \in [N]}$, and user inputs $\{\cI_{i, t}\}_{i \in [N], t \in [T]}$, a view $(\cF, \cM)$ of client $j$ is \textbf{correct} if it satisfies
\begin{multline*}
\cF \cap \{\reg_i\} = \{\reg_k: \exists t \in [T], \trust(\reg_k) = \cI_{j, t}\},    
\end{multline*}
and
\begin{align*}
 \cM = \{(\reg_k, \msg): &\exists t, \send(\reg_j, \msg) = \cI_{k, t} \land \\
 &\exists t' < t, \trust(\reg_j) = \cI_{k, t'} \land \\
 &\exists t'', \trust(\reg_k) = \cI_{j, t''}\}.   
\end{align*}
\end{definition}
\textbf{Remark}: Note that we allow $\cF$ to contain friends that are non-existent. Ruling out these friends is a part of the trust establishment mechanism(\cite[Section 4]{whitepaper}), which is beyond the scope of this paper. 

In the definition, if user $k$ tries to send user $j$ before user $j$ adds user $k$ as a friend, user $j$ should be able to receive the message. This is a feature of our system. 

Since the GUI can query the client at any time, we expect the client's view to be ``correct" all the time. There is a caveat: due to the lack of synchronous rounds, the clients do not immediately read all messages sent by their friends. Thus, the strongest correctness notion of sequential consistency might not be satisfied. Instead, we settle for a pair of weaker consistency models defined in \cite{doug13Consistency}. 
\begin{definition}[Correctness]
\label{defn:correctness}
 We say a messaging scheme is \textbf{correct} if the following consistency model is satisfied with probability $1 - \negl(\lambda)$.
 
 Take any choice of parameters of the honest world experiment, any $j \in [N]$, and any positive integer $T_0 \leq T$. Let $V_j \leftarrow C_j.\mathsf{GetView}()$ be the view of client $j$ after timestep $T_0$ of the Honest World Experiment. Then we must have

1) \textbf{Consistent Prefix}: $V$ is identical to the correct view of the client $j$ if a prefix of user inputs have been executed on each client machine. More formally, with probability $1 - \negl(\lambda)$, for any $j \in [N]$, there exists a map $t: [N] \to [T_0]$ such that  $V$ is a correct view of client $j$ under inputs $(\{\reg_i\}_{i \in [N]}, \{\cI'_{i, t}\})$
where we define
$$\cI'_{i, t} = \begin{cases}
\cI_{i, t}, t \leq t(i) \\
\emptyset, t > t(i)
\end{cases}.$$
2) \textbf{Eventual Consistency}: For any $T_1$, there is a polynomial function $T_{cons}$ in $N, T_1$ such that if $T_0 \geq T_{cons}$, then we can additionally take $t(i) \geq T_1$ for every $i$ with probability $1 - \negl(\lambda)$.
\end{definition}
We now turn to security definitions. We use a real world-ideal world definition of metadata privacy introduced by NIAR.
\begin{figure}[h!]
\begin{framed}
\begin{definition}[Real world Experiment] \hfill\\
\label{defn:real-world}
Same notations as above. Let $\cA$ be a \textbf{stateful} adversary. Let $\cH$ denote the set of honest clients. For convenience, we denote $\reg_{\cH} = \{\reg_i\}_{i \in\cH}$ to be their registration info. The real-world experiment $\mathsf{Real}^{\cA}(1^{\lambda})$ is described below.
\begin{enumerate}
\item $N, T, \cH \leftarrow \cA(1^{\lambda})$.
\item For each $i \in \cH$, $\reg_i \leftarrow C_i.\mathsf{Register}(1^{\lambda}, i, N)$. 
\item $\{\cI_{i, 1}\} \leftarrow \cA(1^{\lambda}, \reg_{\cH})$.
\item For $t$ from $1$ to $T$:
    \begin{enumerate}
    \item For each $i \in H$, $\req_i \leftarrow C_i.\mathsf{Input}(t, \cI_{i, t})$.
    
    \item $\{\resp_i\}_{i \in \cH}, \{\cI_{i, t + 1}\}_{i \in \cH} \leftarrow \cA(1^{\lambda}, \{\req_i\}_{i \in \cH})$.
    
    \item For each $i \in \cH$, $C_i.\mathsf{ServerRPC}(t, \resp_i)$.
    \end{enumerate}
\end{enumerate}
\end{definition}
\end{framed}
\end{figure}

\begin{definition}[Leakage]
Let $\cH \subset [n]$ be a set of honest clients. Let $\{\cI_{i, t}\}_{i \in \cH, t \in [T]}$ be the input from honest clients. We define the \textbf{Leakage} $\mathsf{Leak}(\{\cI_{i, t}\}, \{\reg_{i}\}_{i \in \cH})$ as the set of informations known to compromised clients if the user inputs are executed by a trusted third party. Informally, it contains the time and contents of
\begin{enumerate}
    \item Trust establishment with compromised clients.
    \item Messages sent to the compromised clients.
\end{enumerate}
More formally, given a set $\reg_\cH = \{\reg_{i}\}_{i \in \cH}$ of honest user registration, we define
$$\Leak(\{\cI_{i, t}\}, \reg_\cH) = (\Leak_f, \Leak_m)$$
where
$$\Leak_f = \{(i, \reg, t): \reg \notin \reg_{\cH}, \trust(\reg) = \cI_{i, t}\}.$$
$$\Leak_m = \{(i, \reg, \msg, t): \reg \notin \reg_{\cH}, \mathsf{SendMessage}(\reg, \msg) = \cI_{i, t}\}.$$
\end{definition}
% \arvid{let's use other symbols than $\cF$ and $\cM$ because we use them for the view of a single client already and this is similar but not the same}
% \stzh{Resolved.}
\begin{figure}[h!]
\begin{framed}
\begin{definition}[Ideal World Experiment]
\label{defn:ideal-world}
Same notations as above. Let $\Sim$ be a stateful simulator. The ideal-world experiment $\mathsf{Ideal}^{\cA, \Sim}(1^{\lambda})$ is described below.
\begin{enumerate}
\item $N, T, \cH \leftarrow \cA(1^{\lambda})$.
\item $\reg_{\cH} = \{\reg_i\}_{i \in \cH} \leftarrow \Sim(1^{\lambda}, N, T)$. 
\item $\{\cI_{i, 1}\}_{i \in \cH} \leftarrow \cA(1^{\lambda}, \reg_{\cH})$.
\item For $t$ from $1$ to $T$:
    \begin{enumerate}
    \item  $\{\req_i\}_{i \in \cH} \leftarrow \Sim(t, \mathsf{Leak}(\{\cI_{i, t}\}, \reg_{\cH}))$.
    
    \item $\{\resp_i\}_{i \in \cH}, \{\cI_{i, t + 1}\}_{i \in \cH} \leftarrow \cA(1^{\lambda}, \reg_{\cH})$.
    
    \item $\Sim(t, \{\resp_i\}_{i \in \cH})$.
    \end{enumerate}
\end{enumerate}
\end{definition}
\end{framed}
\end{figure}
\begin{definition}[SIM Security]
\label{defn:SIM-secure}
We say a messaging scheme is SIM-secure iff for  any polynomial upper bounds on $N$ and $T$, there exists a p.p.t simulator $\Sim$ such that for any p.p.t adversary $\cA$, the view of $\cA$ is indistinguishable in $\mathsf{Real}^{\cA}(1^{\lambda})$ and $\mathsf{Ideal}^{\cA, \mathsf{\Sim}}(1^{\lambda})$.
\end{definition}

\todo{SIM-security is a relatively novel form of security defined in NIAR. Because it is not often seen in literature, I'm defining a IND-security below, which should be more traditional.}

\begin{figure}
\begin{framed}
\begin{definition}[Real world IND Experiment] \hfill\\
\label{defn:asphr-ind-real-world}
Same notations as above. The real-world IND-experiment with adversary $\mathsf{Real-IND}^{\cA}(1^{\lambda})$ is described below, parametrized by bit $b \in \{0, 1\}$.
\begin{enumerate}
\item $N, T, \cH \leftarrow \cA(1^{\lambda})$.
\item For each $i \in \cH$, $\reg_i \leftarrow C_i.\mathsf{Register}(1^{\lambda}, i, N)$. 
\item $\{\cI^{0}_{i, 1}\}, \{\cI^{1}_{i, 1}\} \leftarrow \cA(1^{\lambda}, \{\reg_i\}_{i \in \cH})$.
\item For $t$ from $1$ to $T$:
    \begin{enumerate}
    \item For each $i \in H$, $\req_i \leftarrow C_i.\mathsf{Input}(t, \cI^{b}_{i, t})$.
    
    \item for $b'\in \{0, 1\}$, $\{\resp^{b'}_i\}_{i \in \cH},\{\cI^{b'}_{i, t + 1}\}_{i \in \cH} \leftarrow \cA(1^{\lambda}, \{\req_i\}_{i \in \cH})$.
    
    \item For each $i \in \cH$, $C_i.\mathsf{ServerRPC}(t, \resp^{b}_i)$.
    \end{enumerate}
\end{enumerate}
We say an adversary $\cA$ is admissible if with probability $1$, after running the experiment above, we have
\begin{align*}
\Leak(\{\cI^{0}_{i, t}\}_{i \in \cH, t \in [T]}, \{\reg_i\}_{i \in \cH}) \\
= \Leak(\{\cI^{1}_{i, t}\}_{i \in \cH, t \in [T]}, \{\reg_i\}_{i \in \cH}).    
\end{align*}
\end{definition}
\end{framed}
\end{figure}
\begin{definition}[IND Security]
We say a messaging scheme is IND-secure iff for any polynomial upper bounds on $N$ and $T$ and any admissible adversary $\cA$, its view in the real world IND-experiment for $b = 0$ and $b = 1$ are indistinguishable.
\end{definition}

\todo{Check that SIM secure and IND secure are equivalent. I'm fairly convinced by NIAR Appendix A.}
Finally, we require the messaging scheme to satisfy Integrity: while a malicious server can DoS users, it shouldn't be able to forge messages between honest users, or selectively drop messages between honest users. In other words, consistent prefix must still be satisfied even though we cannot guarantee eventual consistency. We define the notion of integrity rigorously.
\begin{definition}[Integrity]
\label{defn:integrity}
Same notation as before. Consider the real world experiment described in \cref{defn:real-world}. Take any pair of honest users $i, j \in \cH$, and consider the view of $j$ at the end of the experiment given by $V_j = (\cF, \cM) \leftarrow C_j.\mathsf{GetView}()$.  Then we say the messaging scheme guarantees Integrity if with probability $1 - \negl(\lambda)$, there exists a $t(i)$ such that
\begin{multline*}
     \{\msg : (\reg_i, \msg) \subset \cM\} \\
      = \{(\reg_k, \msg): \exists t \leq t(i), \send(\reg_j, \msg) = \cI_{k, t} \land \\
             \exists t' < t, \trust(\reg_j) = \cI_{k, t'} \land \\
             \exists t'', \trust(\reg_k) = \cI_{j, t''}\}. \\   
\end{multline*}
\end{definition}