\newcommand{\Leak}{\mathsf{Leak}}
\section{General Definitions}
\todo{User = person using the application. Client = the application the user is using}

In this section, we attempt to design a security definition for a messaging application that is as general as possible, then outline some properties that we expect a satisfactory metadata-private messaging app to achieve. We start from the following basic assumptions.
\begin{enumerate}
    \item The messaging app has a centralized server in charge of storing and routing messages.
    \item The messaging app has a large number of users, interacting with client machines. The client machine should allow the user to register, add friends, and send messages. It should display received messages to the user.
    \item To achieve metadata privacy, the messaging application should hide metadata of conversations between honest users from a powerful adversary that controls the server and a subset of clients. 
\end{enumerate}
\begin{definition}
In this document, we divide our execution into discrete \textbf{timesteps}. In reality, each timestep might be a small unit of time, like 100ms. This is different from ``rounds" used in most MPM security definitions, since clients do not necessarily transmit real or fake messages at every timestep.
\end{definition}
\begin{definition}
The \textbf{view} of a client is a tuple consisting of

1. A list of friends $\cF$ of the client.

2. A list of messages $\cM$ received by the client, including the sender, timestep and content of the messages. \todo{Do we reveal whether the message have been received?}
\end{definition}
\begin{definition}
A \textbf{user input} is a command the user can issue to the client. It can take one of the following values.

$\emptyset$: noop.

$\trust(\reg)$: Add the user identified by $\reg$ as a friend, and enable the two parties to start a conversation.

$\send(\reg, \msg)$: Send the message $\msg$ to the client identified by $\reg$. We assume that the messages have a fixed length $L_{\msg}$ polynomial in $\lambda$.

\todo{We could do a ``setparameter" method here to allow clients to customize their transmission schedule, etc.}

Without loss of generality, we assume the client issues one input per timestep.
\end{definition}
\textbf{Remark}:  To support variable length messages, we simply pad short messages, and split long messages into chunks of length $L_{\msg}$. This does not affect our security definition below.
\begin{definition}
\label{def:messaging-scheme}
A \textbf{Messaging Scheme/Application/App} consists of the following polynomial time methods on stated Turing machines.

Client Side Algorithms for client $C$.
\begin{itemize}
    \item $C.\mathsf{Register}(1^{\lambda}, i, N) \to \reg$, takes in a security parameter $\lambda$, the index $i$ of the client, the total number of users $N$, and outputs a public registration info $\reg$ and initializes the client database $D_C$.
    
    \item $C.\mathsf{Input}(t, \cI) \to \req$, takes in a user input $\cI$, updates the local storage to reflect that, and issues a request $\req$ to the server. The request can be empty, which means the client doesn't request the server on that timestep.
    
    \item $C.\mathsf{ServerRPC}(t, \resp)\to \lambda$, updates the database given the server response $\resp$.
    
    \item $C.\mathsf{GetView}() \to V$, outputs the view of the client.
\end{itemize}

Server Side Algorithms for server $S$.

\begin{itemize}
    \item $S.\mathsf{InitServer}(1^{\lambda}, N)$, takes in the security parameter $\lambda$, number of clients $N$, and initializes the server side database $D_S$.
    \item $S.\mathsf{ClientRPC}(t, \{\req_i\}_{i = 1}^N) \to \{\resp_i\}_{i = 1}^N$, responds to all client requests of the timestep.
\end{itemize}
\end{definition}
We now define how the algorithm will be executed, and correctness.
\begin{figure}[h!]
\begin{framed}
\begin{definition}[Honest Server Experiment]  \hfill\\
\label{defn:honest-server-experiment}
\textbf{Parameters}: \begin{enumerate}
    \item $\lambda$, the security parameter.
    \item $N$, the number of clients, bounded above by a polynomial function $N(\lambda)$ of $\lambda$.
    \item $T$, the number of timesteps, bounded above by a polynomial function $T(\lambda)$ of $\lambda$.
    \item For each client $i$ and timestep $t$, a set of user inputs $\cI_{i, t}$, whose length is bounded by a constant.
\end{enumerate}

Let $S$ denote the server machine, let $\{C_i\}_{i = 1}^N$ denote the client machines.

\textbf{Execution}:
\begin{enumerate}
\item $S.\mathsf{InitServer}(1^{\lambda}, N)$. 
\item For each $i \in [N]$, $\reg_i \leftarrow C_i.\mathsf{Register}(1^{\lambda}, i, N)$. 
\item For $t$ from $1$ to $T$:
    \begin{enumerate}
    \item For each $i \in [N]$, $\req_i \leftarrow C_i.\mathsf{Input}(t, \cI)$.
    
    \item $\{\resp_i\} \leftarrow S.\mathsf{ClientRPC}(t, \{\req_i\})$.
    
    \item For each $i \in [N]$, $C_i.\mathsf{ServerRPC}(t, \resp_i)$.
    \end{enumerate}
\end{enumerate}
\end{definition}
\end{framed}
\end{figure}

\arvid{Do we want an adversary to be able to control some of the clients? Ideally I think we would want to, because we want to guarantee resistance against denial of service attacks from clients}
\stzh{Good idea. I'll come back to this after I finish the security proof in the current iteration.}

The definition of correctness is a bit tricky: we do not immediately read all messages sent by our friends. Instead, we settle for a pair of weaker consistency models defined in 6.033. %https://www.microsoft.com/en-us/research/wp-content/uploads/2011/10/ConsistencyAndBaseballReport.pdf
\begin{definition}
We define the ``correct" view given user inputs. Informally speaking, the correct view should satisfy
\begin{enumerate}
    \item The list of friends should contain the friends we called $\trust$ on.         
    \item Messages from a client with established trust should be present.  
    \item No other friends or messages should be present. 
\end{enumerate}
More formally, given a set of clients identified by $\{\reg_i\}_{i \in [N]}$, and user inputs $\{\cI_{i, t}\}_{i \in [N], t \in [T]}$, a view $(\cF, \cM)$ of client $j$ is said to be \textbf{correct} if it satisfies
\begin{multline*}
\cF \cap \{\reg_i: i \in [N]\} = 
 \\ \{\reg_k: \exists t \in [T], \trust(\reg_k) = \cI_{j, t}\},    
\end{multline*}
and
\begin{multline*}
 \cM = \{(\reg_k, \msg): \exists t, \send(\reg_j, \msg) = \cI_{k, t} \land \\
 \exists t' < t, \trust(\reg_j) = \cI_{k, t'} \land \\
 \exists t'', \trust(\reg_k) = \cI_{j, t''}\} .   
\end{multline*}
\end{definition}
\textbf{Remark}: Note that we allow $\cF$ to contain friends that are non-existent. This is because we need a trusted PKI to verify the identity of the client, which we do not have. I do not think having these non-existent friends would cause any harm...

Of course, we have a handshake protocol to verify the validity of friends, but this is a preliminary draft, so I don't know if it is appropriate to go so far right now...

Also note that $k$ might be broadcasting the message before $j$ adds $k$ as a friend. Our system is actually capable of receiving such messages.
\begin{definition}[Correctness]
\label{defn:correctness}
 We say a messaging scheme is \textbf{correct} if the following consistency model is satisfied with probability $1 - \negl(\lambda)$.
 
 Take any choice of parameters of the honest world experiment, any index $i \in [N]$, and any time $T_0$. Let $V_j \leftarrow C_j.\mathsf{GetView}()$ be the view of client $j$ after timestep $T_0$ of the Honest World Experiment. Then we want

1) \textbf{Consistent Prefix}: $V$ is identical to the correct view of the client $j$ if a prefix of user inputs have been executed on each machine. More formally, with probability $1 - \negl(\lambda)$, for any $j \in [N]$, there exists a map $t: [N] \to [T]$ such that  $V$ is a correct view of client $j$ under inputs $(\{\reg_i\}_{i \in [N]}, \{\cI'_{i, t}\})$
where we define
$$\cI'_{i, t} = \begin{cases}
\cI_{i, t}, t \leq t(i) \\
\emptyset, t > t(i)
\end{cases}.$$
2) \textbf{Eventual Consistency}: For any $T_1$, there is a polynomial function $T_{cons}$ in $N, T_1$ such that if $T_0 \geq T_{cons}$, then we can additionally take $t(i) \geq T_1$ for every $i$ with probability $1 - \negl(\lambda)$.
\end{definition}
We now turn to security definitions. We use the real world-ideal world definition of metadata privacy.
\begin{figure}[h!]
\begin{framed}
\begin{definition}[Real world Experiment] \hfill\\
\label{defn:real-world}
Same notations as above. Let $\cA$ be a \textbf{stateful} adversary. Let $\cH$ denote the set of honest clients. For convenience, we denote $\reg_{\cH} = \{\reg_i\}_{i \in\cH}$ to be their registration info. The real-world experiment $\mathsf{Real}^{\cA}(1^{\lambda})$ is described below.
\begin{enumerate}
\item $N, T, \cH \leftarrow \cA(1^{\lambda})$.
\item For each $i \in \cH$, $\reg_i \leftarrow C_i.\mathsf{Register}(1^{\lambda}, i, N)$. 
\item $\{\cI_{i, 1}\} \leftarrow \cA(1^{\lambda}, \reg_{\cH})$.
\item For $t$ from $1$ to $T$:
    \begin{enumerate}
    \item For each $i \in H$, $\req_i \leftarrow C_i.\mathsf{Input}(t, \cI_{i, t})$.
    
    \item $\{\resp_i\}_{i \in \cH}, \{\cI_{i, t + 1}\}_{i \in \cH} \leftarrow \cA(1^{\lambda}, \{\req_i\}_{i \in \cH})$.
    
    \item For each $i \in \cH$, $C_i.\mathsf{ServerRPC}(t, \resp_i)$.
    \end{enumerate}
\end{enumerate}
\end{definition}
\end{framed}
\end{figure}

\begin{definition}[Leakage]
Let $\cH \subset [n]$ be a set of honest clients. Let $\{\cI_{i, t}\}_{i \in \cH, t \in [T]}$ be the input from honest clients. We define the \textbf{Leakage} $\mathsf{Leak}(\{\cI_{i, t}\}, \{\reg_{i}\}_{i \in \cH})$ as the set of informations known to compromised clients if the user inputs are executed by a trusted third party. Informally, it contains the time and contents of
\begin{enumerate}
    \item Trust establishment with compromised clients.
    \item Messages sent to the compromised clients.
\end{enumerate}
More formally, given a set $\reg_\cH = \{\reg_{i}\}_{i \in \cH}$ of honest user registration, we define
$$\Leak(\{\cI_{i, t}\}, \reg_\cH) = (\Leak_f, \Leak_m)$$
where
$$\Leak_f = \{(i, \reg, t): \reg \notin \reg_{\cH}, \trust(\reg) = \cI_{i, t}\}.$$
$$\Leak_m = \{(i, \reg, \msg, t): \reg \notin \reg_{\cH}, \mathsf{SendMessage}(\reg, \msg) = \cI_{i, t}\}.$$
\end{definition}
% \arvid{let's use other symbols than $\cF$ and $\cM$ because we use them for the view of a single client already and this is similar but not the same}
% \stzh{Resolved.}
\begin{figure}[h!]
\begin{framed}
\begin{definition}[Ideal World Experiment]
\label{defn:ideal-world}
Same notations as above. Let $\Sim$ be a stateful simulator. The ideal-world experiment $\mathsf{Ideal}^{\cA, \Sim}(1^{\lambda})$ is described below.
\begin{enumerate}
\item $N, T, \cH \leftarrow \cA(1^{\lambda})$.
\item $\reg_{\cH} = \{\reg_i\}_{i \in \cH} \leftarrow \Sim(1^{\lambda}, N, T)$. 
\item $\{\cI_{i, 1}\}_{i \in \cH} \leftarrow \cA(1^{\lambda}, \reg_{\cH})$.
\item For $t$ from $1$ to $T$:
    \begin{enumerate}
    \item  $\{\req_i\}_{i \in \cH} \leftarrow \Sim(t, \mathsf{Leak}(\{\cI_{i, t}\}, \reg_{\cH}))$.
    
    \item $\{\resp_i\}_{i \in \cH}, \{\cI_{i, t + 1}\}_{i \in \cH} \leftarrow \cA(1^{\lambda}, \reg_{\cH})$.
    
    \item $\Sim(t, \{\resp_i\}_{i \in \cH})$.
    \end{enumerate}
\end{enumerate}
\end{definition}
\end{framed}
\end{figure}
\begin{definition}[SIM Security]
\label{defn:SIM-secure}
We say a messaging scheme is SIM-secure iff for  any polynomial upper bounds on $N$ and $T$, there exists a p.p.t simulator $\Sim$ such that for any p.p.t adversary $\cA$, the view of $\cA$ is indistinguishable in $\mathsf{Real}^{\cA}(1^{\lambda})$ and $\mathsf{Ideal}^{\cA, \mathsf{\Sim}}(1^{\lambda})$.
\end{definition}

\todo{SIM-security is a relatively novel form of security defined in NIAR. Because it is not often seen in literature, I'm defining a IND-security below, which should be more traditional.}

\begin{figure}
\begin{framed}
\begin{definition}[Real world IND Experiment] \hfill\\
\label{defn:ind-real-world}
Same notations as above. The real-world IND-experiment with adversary $\mathsf{Real-IND}^{\cA}(1^{\lambda})$ is described below, parametrized by bit $b \in \{0, 1\}$.
\begin{enumerate}
\item $N, T, \cH \leftarrow \cA(1^{\lambda})$.
\item For each $i \in \cH$, $\reg_i \leftarrow C_i.\mathsf{Register}(1^{\lambda}, i, N)$. 
\item $\{\cI^{0}_{i, 1}\}, \{\cI^{1}_{i, 1}\} \leftarrow \cA(1^{\lambda}, \{\reg_i\}_{i \in \cH})$.
\item For $t$ from $1$ to $T$:
    \begin{enumerate}
    \item For each $i \in H$, $\req_i \leftarrow C_i.\mathsf{Input}(t, \cI^{b}_{i, t})$.
    
    \item for $b'\in \{0, 1\}$, $\{\resp^{b'}_i\}_{i \in \cH},\{\cI^{b'}_{i, t + 1}\}_{i \in \cH} \leftarrow \cA(1^{\lambda}, \{\req_i\}_{i \in \cH})$.
    
    \item For each $i \in \cH$, $C_i.\mathsf{ServerRPC}(t, \resp^{b}_i)$.
    \end{enumerate}
\end{enumerate}
We say an adversary $\cA$ is admissible if with probability $1$, after running the experiment above, we have
\begin{align*}
\Leak(\{\cI^{0}_{i, t}\}_{i \in \cH, t \in [T]}, \{\reg_i\}_{i \in \cH}) \\
= \Leak(\{\cI^{1}_{i, t}\}_{i \in \cH, t \in [T]}, \{\reg_i\}_{i \in \cH}).    
\end{align*}
\end{definition}
\end{framed}
\end{figure}
\begin{definition}[IND Security]
We say a messaging scheme is IND-secure iff for any polynomial upper bounds on $N$ and $T$ and any admissible adversary $\cA$, its view in the real world IND-experiment for $b = 0$ and $b = 1$ are indistinguishable.
\end{definition}

\todo{Check that SIM secure and IND secure are equivalent. I'm fairly convinced by NIAR Appendix A.}
Finally, we require the messaging scheme to satisfy Integrity: while a malicious server can DoS users, it shouldn't be able to forge messages between honest users, or selectively drop messages between users. In other words, consistent prefix must still be satisfied even though we cannot guarantee eventual consistency. We define the notion of integrity rigorously.
\begin{definition}[Integrity]
\label{defn:integrity}
Same notation as before. Consider the real world experiment described in \cref{defn:real-world}. Take any pair of honest users $i, j \in \cH$, and consider the view of $j$ at the end of the experiment given by $V_j = (\cF, \cM) \leftarrow C_j.\mathsf{GetView}()$.  Then we say the messaging scheme guarantees Integrity if with probability $1 - \negl(\lambda)$, there exists a $t(i)$ such that
\begin{multline*}
     \{\msg : (\reg_i, \msg) \subset \cM\} \\
      = \{(\reg_k, \msg): \exists t \leq t(i), \send(\reg_j, \msg) = \cI_{k, t} \land \\
             \exists t' < t, \trust(\reg_j) = \cI_{k, t'} \land \\
             \exists t'', \trust(\reg_k) = \cI_{j, t''}\}. \\   
\end{multline*}
\end{definition}